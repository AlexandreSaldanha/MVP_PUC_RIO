{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMks3Tz3j4bbcuJXxJ+EQS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexandreSaldanha/MVP_PUC_RIO/blob/main/Diabetes_dos_%C3%ADndios_Pima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hckuqm-5cToF"
      },
      "outputs": [],
      "source": [
        "# configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports necessários\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga do Dataset"
      ],
      "metadata": {
        "id": "RuLblkeDc2RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Informa a URL de importação do dataset\n",
        "url = \"https://raw.githubusercontent.com/AlexandreSaldanha/MVP_PUC_RIO/refs/heads/main/diabetes\"\n",
        "\n",
        "# Lê o arquivo\n",
        "dataset = pd.read_csv(url, delimiter=',')\n",
        "\n",
        "# Mostra as primeiras linhas do dataset\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "CCyOu9GGcZtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separação em conjunto de treino e conjunto de teste com holdout"
      ],
      "metadata": {
        "id": "-TKf4X28cvVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = 0.20 # tamanho do conjunto de teste\n",
        "seed = 7 # semente aleatória\n",
        "\n",
        "# Separação em conjuntos de treino e teste\n",
        "array = dataset.values\n",
        "X = array[:,0:8]\n",
        "y = array[:,8]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "    test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratificação\n",
        "\n",
        "# Parâmetros e partições da validação cruzada\n",
        "scoring = 'accuracy'\n",
        "num_particoes = 10\n",
        "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # validação cruzada com estratificação"
      ],
      "metadata": {
        "id": "lVOj1ebicl0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem e Inferência"
      ],
      "metadata": {
        "id": "1HJG9s1Tc7qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação e avaliação de modelos: linha base"
      ],
      "metadata": {
        "id": "lieUql2qdL2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Lista que armazenará os modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os parâmetros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(bases)))\n",
        "\n",
        "# Listas para armazenar os resultados\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Avaliação dos modelos\n",
        "for name, model in models:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "fig.suptitle('Comparação dos Modelos')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0rKU6B4tdOf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação e avaliação de modelos: dados padronizados e normalizados"
      ],
      "metadata": {
        "id": "Tl1vpFrMdNtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "# Listas para armazenar os armazenar os pipelines e os resultados para todas as visões do dataset\n",
        "pipelines = []\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "\n",
        "# Criando os elementos do pipeline\n",
        "\n",
        "# Algoritmos que serão utilizados\n",
        "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "cart = ('CART', DecisionTreeClassifier())\n",
        "naive_bayes = ('NB', GaussianNB())\n",
        "svm = ('SVM', SVC())\n",
        "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
        "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
        "voting = ('Voting', VotingClassifier(bases))\n",
        "\n",
        "# Transformações que serão utilizadas\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "\n",
        "# Montando os pipelines\n",
        "\n",
        "# Dataset original\n",
        "pipelines.append(('LR-orig', Pipeline([reg_log])))\n",
        "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
        "pipelines.append(('CART-orig', Pipeline([cart])))\n",
        "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
        "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
        "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
        "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
        "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
        "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
        "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
        "\n",
        "# Dataset Padronizado\n",
        "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log])))\n",
        "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
        "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
        "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
        "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
        "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
        "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
        "\n",
        "# Dataset Normalizado\n",
        "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log])))\n",
        "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
        "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
        "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
        "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
        "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
        "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
        "\n",
        "# Executando os pipelines\n",
        "for name, model in pipelines:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "fig.suptitle('Comparação dos Modelos - Dataset orginal, padronizado e normalizado')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names, rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KcoSLVC6depz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otimização dos hiperparâmetros"
      ],
      "metadata": {
        "id": "CnVWtfUJdrPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning do KNN\n",
        "\n",
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "pipelines = []\n",
        "\n",
        "# Definindo os componentes do pipeline\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "pipelines.append(('knn-orig', Pipeline(steps=[knn])))\n",
        "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
        "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
        "\n",
        "param_grid = {\n",
        "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
        "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
        "}\n",
        "\n",
        "# Prepara e executa o GridSearchCV\n",
        "for name, model in pipelines:\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))"
      ],
      "metadata": {
        "id": "semz7w27dt5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalização do Modelo"
      ],
      "metadata": {
        "id": "yP8rewOld71-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Avaliação do modelo com o conjunto de testes\n",
        "\n",
        "# Preparação do modelo\n",
        "scaler = StandardScaler().fit(X_train) # ajuste do scaler com o conjunto de treino\n",
        "rescaledX = scaler.transform(X_train) # aplicação da padronização no conjunto de treino\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(rescaledX, y_train)\n",
        "\n",
        "# Estimativa da acurácia no conjunto de teste\n",
        "rescaledTestX = scaler.transform(X_test) # aplicação da padronização no conjunto de teste\n",
        "predictions = model.predict(rescaledTestX)\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVk5eTMCd9JE",
        "outputId": "91c1cad6-8369-4baf-971d-d324b7ff8d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7727272727272727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparação do modelo com TODO o dataset\n",
        "scaler = StandardScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
        "rescaledX = scaler.transform(X) # aplicação da padronização com TODO o dataset\n",
        "model.fit(rescaledX, y)"
      ],
      "metadata": {
        "id": "2KTe2s0neFcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulando a aplicação do modelo em dados não vistos"
      ],
      "metadata": {
        "id": "7cqqmFTreP4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Novos dados - não sabemos a classe!\n",
        "data = {'preg':  [1, 9, 5],\n",
        "        'plas': [90, 100, 110],\n",
        "        'pres': [50, 60, 50],\n",
        "        'skin': [30, 30, 30],\n",
        "        'test': [100, 100, 100],\n",
        "        'mass': [20.0, 30.0, 40.0],\n",
        "        'pedi': [1.0, 2.0, 1.0],\n",
        "        'age': [15, 40, 40],\n",
        "        }\n",
        "\n",
        "atributos = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "entrada = pd.DataFrame(data, columns=atributos)\n",
        "\n",
        "array_entrada = entrada.values\n",
        "X_entrada = array_entrada[:,0:8].astype(float)\n",
        "\n",
        "# Padronização nos dados de entrada usando o scaler utilizado em X\n",
        "rescaledEntradaX = scaler.transform(X_entrada)\n",
        "print(rescaledEntradaX)"
      ],
      "metadata": {
        "id": "m1PkZGTDeRzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição de classes dos dados de entrada\n",
        "saidas = model.predict(rescaledEntradaX)\n",
        "print(saidas)"
      ],
      "metadata": {
        "id": "vGSfVxcCeXq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}